
scala> val bankDF = spark.read.option("inferSchema", "True").option("header", "True").csv("project/project1_outputfile.csv")
21/08/22 06:48:41 WARN lineage.LineageWriter: Lineage directory /var/log/spark/lineage doesn't exist or is not writable. Lineage for this application will be disabled.
bankDF: org.apache.spark.sql.DataFrame = [age: int, job: string ... 15 more fields]

scala> import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.DataFrame
scala> case class bank(age:Int, job:String, marital:String, education:String, defaultn:String, balance:Int, housing:String, loan:String, contact:String, day:Int, month: String, duration:Int, campaign:Int, pdays:Int, previous:Int, poutcome:String, y:String)
defined class bank



scala> bankDF.createOrReplaceTempView("bank")

scala> 

scala> val marketing_success_rate=spark.sql("select count(case when y= 'yes' then 1 end)/count(*)*100  as marketing_success_rate from bank").show()
+----------------------+                                                        
|marketing_success_rate|
+----------------------+
|    11.698480458295547|
+----------------------+

marketing_success_rate: Unit = ()

scala> 

scala> val marketing_fail_rate=spark.sql("select count(case when y='no' then 1 end)/count(*)*100  as marketing_fail_rate from bank").show()
+-------------------+
|marketing_fail_rate|
+-------------------+
|  88.30151954170445|
+-------------------+

marketing_fail_rate: Unit = ()

scala> 

scala> val maximum_age=spark.sql("select max(age) as maximum_age from bank").show() 
+-----------+
|maximum_age|
+-----------+
|         95|
+-----------+

maximum_age: Unit = ()

scala> 

scala> val mean_age=spark.sql("select avg(age) as mean_age from bank").show()
+-----------------+
|         mean_age|
+-----------------+
|40.93621021432837|
+-----------------+

mean_age: Unit = ()

scala> val max_mean_min_age=spark.sql("select max(age) as maxmium_age, avg(age) as mean_age, min(age) as minimum_age from bank").show()

scala> val min_age=spark.sql("select min(age)  as minimum_age from bank").show()
+-----------+
|minimum_age|
+-----------+
|         18|
+-----------+

min_age: Unit = ()


scala> import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SQLContext

scala> import org.apache.spark.sql.functions.mean
import org.apache.spark.sql.functions.mean

scala> val ssb=new org.apache.spark.sql.SparkSession.Builder()
ssb: org.apache.spark.sql.SparkSession.Builder = org.apache.spark.sql.SparkSession$Builder@2e897323

scala> val sparkSession=ssb.getOrCreate()
sparkSession: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@385abc08

scala> val sqlCtx=sparkSession.sqlContext;
sqlCtx: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@63cb06df

scala> val ageRDD = sqlCtx.udf.register("ageRDD",(age:Int) => { if (age < 20)"Teen"else if (age > 20 && age <= 32) "Young"else if (age > 33 && age <= 55) "Middle Aged"else "Old"})
21/08/30 07:19:03 WARN analysis.SimpleFunctionRegistry: The function agerdd replaced a previously registered function.
ageRDD: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(IntegerType)))

scala> val banknewDF = bankDF.withColumn("age",ageRDD(bankDF("age")))
banknewDF: org.apache.spark.sql.DataFrame = [age: string, job: string ... 15 more fields]

scala> banknewDF.createOrReplaceTempView("bank_new")



scala> val age_target = spark.sql("select age, count(*) as number from bank_new where y='yes' group by age order by number desc ").show( )
+-----------+------+                                                            
|        age|number|
+-----------+------+
|Middle Aged|  2601|
|      Young|  1539|
|        Old|  1131|
|       Teen|    18|
+-----------+------+

age_target: Unit = ()

scala> val ageInd = new org.apache.spark.ml.feature.StringIndexer().setInputCol("age").setOutputCol("ageIndex")
ageInd: org.apache.spark.ml.feature.StringIndexer = strIdx_23d1330bafe6

scala> var strIndModel = ageInd.fit(banknewDF)
strIndModel: org.apache.spark.ml.feature.StringIndexerModel = strIdx_23d1330bafe6

scala> strIndModel.transform(banknewDF).select("age","ageIndex").show(5)
+-----------+--------+
|        age|ageIndex|
+-----------+--------+
|        Old|     2.0|
|Middle Aged|     0.0|
|        Old|     2.0|
|Middle Aged|     0.0|
|        Old|     2.0|
+-----------+--------+
only showing top 5 rows